# This script will read 1-year Landsat observations Array Image dataset generated by 'ExtractLandsatAIplus_xScene.py'
# script. The data is then processed to build sequences proper for feeding a LSTM trained classifier models, and
# the output map is saved as a numpy array, a .png image, and a georeferenced .tif image.
#
# Update V3.0:
#       - Adding possibility of limiting temporal range when doing inference and dropping any of
#         Landsat 5, 7, or 8 sensors.
#       - Manually deleting rows and columns to adjust the position of the output map
#       - Dropping any observation after max_len entries
#       - Saving class probabilities and classified map numpy array
############################################################################################################
# Shahriar S. Heydari, 12/15/2019

import os, datetime,sys
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'	# 0: default, 1: no INFO, 2: no INFO and WARNING, 3: no INFO, WARNING, and ERROR printed
import numpy as np
import time, glob, json
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from osgeo import gdal
from PIL import Image
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.compat.v1.keras import backend as K
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import Session
config = ConfigProto()
config.gpu_options.allow_growth = True
sess = Session(config=config)
K.set_session(sess)

# variables to set before execution
inFolder = ''           # Folder where input TFrecord gz files are placed
blockAIfile = 'xxxx_AI'   # common part for the TFRecord gz files
modelFolder = ''        # Folder where trained model is placed
modelFile = 'FH_0_best_model.h5'   # model file
outFolder = ''         # Folder where generated output files will be placed
outFileName = outFolder+'xxxx_FH_0'   # This is the common part for the output file names

# below variables are not changed normally
projectionSource = '' # if not given, read from json file
sensors = [5,7,8]       # Select any of landsat 5, 7, or 8 (multiple selection is also allowed)
reduceTime = 0      # if 0, full year sdequences are used,
                    # If 1, the time range between the first and second elements of selectDOY are used,
#                   # if 2, the observation closest to selectDOY elements are used.
selectDOY = []  # Approximate mid season DOYs: 15(winter), 106, 197, 288(fall)
max_len = 100
LSbands = ['blue','green','red','NIR','SWIR1','SWIR2']
staticBands = ['ECO_ID','elevation', 'slope', 'aspect']
annualBands = ['temp_Jan', 'temp_Feb', 'temp_Mar', 'temp_Apr', 'temp_May', 'temp_Jun',
                   'temp_Jul', 'temp_Aug', 'temp_Sep', 'temp_Oct', 'temp_Nov', 'temp_Dec',
                   'rain_Jan', 'rain_Feb', 'rain_Mar', 'rain_Apr', 'rain_May', 'rain_Jun',
                   'rain_Jul', 'rain_Aug', 'rain_Sep', 'rain_Oct', 'rain_Nov', 'rain_Dec']
bands = allBands = []  # will be reassigned later
NC = 7                      # Number of classes
DOY_max = 366.0
sensor_max = 10.0
ECOID_max = 850.0
elevation_min = -20         # meter
elevation_max = 6500.0      # meter
slope_max = 360.0           # degree
aspect_max = 360.0          # degree
DD_max = 2.0
GLCM_diss_max = 100.0
GLCM_ent_max = 10.0
GLCM_svag_max = 250.0
GLCM_var_max = 1500.0
temp_min = -30.0            # degree centigrade
temp_max = 50.0             # degree centigrade
rain_max = 2650.0           # mm
Nrain_max = 12000.0   # mm
bands = []              # dynamic (changing by each observation per pixel), assigned later
divide_to = []          # dynamic bands scaling factor (assigned later)
add_to = []             # dynamic bands scaling factor (asigned later)
sBands = []             # static bands (always constant per pixel)
divide_to_s = []
add_to_s = []
ssBands = []            # annual bands (constant during each year per pixel)
divide_to_ss = []
add_to_ss = []
allBands = []           # aggregate of all above bands
divide_to_all = []
add_to_all = []
LSbandsIndex = []
CNNbandsIndex = []
elevBandIndex = 0

def closest(lst, K):
    lst = np.asarray(lst)
    idx = (np.abs(lst - K)).argmin()
    return idx

def setBandScaling(allBands):
    global divide_to, add_to, add_to_s, divide_to_s, add_to_ss, divide_to_ss, divide_to_all, add_to_all, \
        LSbandsIndex, CNNbandsIndex,elevBandIndex

    LSbandsIndex = [bands.index(x) for x in LSbands]
    elevBandIndex = sBands.index('elevation')
    divide_to_all = np.ones((len(allBands),1,1))
    add_to_all = np.zeros((len(allBands),1,1))
    for select in allBands:
        i = allBands.index(select)
        if select == 'DOY':
            divide_to_all[i,] = DOY_max
        elif select == 'sensor':
            divide_to_all[i,] = sensor_max
        elif select == 'ECO_ID':
            divide_to_all[i,] = ECOID_max
        elif select == 'elevation':
            divide_to_all[i,] = elevation_max - elevation_min
            add_to_all[i,] = -elevation_min
        elif select == 'slope':
            divide_to_all[i,] = slope_max
        elif select == 'aspect':
            divide_to_all[i,] = aspect_max
        elif select in ['NDVI','MSAVI2','SATVI','NLI','BSI','MNDWI','WNDWI','NDBI','ENDISI']:
            add_to_all[i,] = 1
        elif select == 'DD':
            divide_to_all[i,] = DD_max
        elif 'diss' in select:
            divide_to_all[i,] = GLCM_diss_max
        elif 'ent' in select:
            divide_to_all[i,] = GLCM_ent_max
        elif 'savg' in select:
            divide_to_all[i,] = GLCM_svag_max
        elif 'var' in select:
            divide_to_all[i,] = GLCM_var_max
        elif 'temp' in select:
            divide_to_all[i,] = temp_max - temp_min
            add_to_all[i,] = -temp_min
        elif 'Nrain' in select:
            divide_to_all[i,] = Nrain_max
        elif 'rain' in select:
            divide_to_all[i,] = rain_max
        divide_to = divide_to_all[:len(bands)]
        add_to = add_to_all[:len(bands)]
        divide_to_ss = divide_to_all[len(bands):len(bands)+len(ssBands),:,:]
        add_to_ss = add_to_all[len(bands):len(bands)+len(ssBands),:,:]
        divide_to_s = divide_to_all[len(bands)+len(ssBands):,:,:]
        add_to_s = add_to_all[len(bands)+len(ssBands):,:,:]

########################################################################################################
print('Processing block ',blockAIfile)
start_time = time.time()
configFile = inFolder + blockAIfile + 'timeTags_.csv'
if not os.path.exists(configFile):
    configFile = inFolder + blockAIfile + 'timeTags_ee_export.csv'
jsonFile = inFolder + blockAIfile + '.json'
if not os.path.exists(jsonFile):
    jsonFile = inFolder + blockAIfile + 'mixer.json'
dataFiles = sorted(glob.glob(inFolder + blockAIfile + '*tfrecord.gz'))

with open(jsonFile, 'r') as json_file:
    cont = json_file.read()
contents = json.loads(cont)
if projectionSource == '':
    proj = contents['projection']['crs']
    c = contents['projection']['affine']['doubleMatrix']
    crs = tuple([c[i] for i in [2,0,1,5,3,4]])
else:
    ref = gdal.Open(projectionSource)
    proj = ref.GetProjection()
    crs = ref.GetGeoTransform()
    del ref
patchSize = contents['patchDimensions'][0]
patchPerRow = contents['patchesPerRow']
totalPatches = contents['totalPatches']
patchPerCol = int(totalPatches / patchPerRow)
imageWidth = patchSize * patchPerRow
imageHeight = patchSize * patchPerCol

with open(configFile, 'r') as config:
    contents = [x.strip() for x in config.readlines()]
bands = [x.replace(',', '').replace('"[', '').replace(']"', '').strip() for x in contents[1].split()]  # sep=',')]
numBands = len(bands)
contents = contents[2:]
valid_doy = np.zeros(len(contents))
for i in range(len(contents)):
    scene = contents[i]
    if (('LT05' in scene) and (5 in sensors)) or (('LE07' in scene) and (7 in sensors)) or \
            (('LC08' in scene) and (8 in sensors)):
        valid_doy[i] = 1
if not valid_doy.any():
    print('All landsat scenes are filtered by limiting sensor to ',sensors)
    sys.exit(0)
num_scenes = len(contents)
num_features = num_scenes * numBands
dates = np.array([int(x[-8:]) for x in contents])
doys = []
dates = [str(i) for i in dates]
years = [int(date[0:4]) for date in dates]
months = [int(date[4:6]) for date in dates]
days = [int(date[6:8]) for date in dates]
for i in range(len(years)):
    doys.append((datetime.date(years[i], months[i], days[i]) - datetime.date(years[i], 1, 1)).days + 1)
doys = np.array(doys)
if reduceTime == 2:
    doys_index = np.unique([closest(doys, l) for l in selectDOY])
elif reduceTime == 1:
    start = np.argwhere(doys - selectDOY[0] >= 0)
    if len(start) == 0:
        doys_index = []
    else:
        start = start[0]
        end = np.argwhere(selectDOY[1] -doys < 0)
        if len(end) == 0:
            end = len(doys)-1
        else:
            end = end[0]-1
        doys_index = np.arange(start, end+1)
else:
    doys_index = np.arange(len(doys))
# filter doys according to sensor
doys_index = [doys_index[i] for i in range(len(doys_index)) if valid_doy[doys_index[i]] > 0]
if not doys_index:
    print('All landsat scenes are filtered by limiting time and sensor.')
    sys.exit(0)
print('Landsat {} sensor(s) are selected. Selected DOYs are: {}'.format(sensors,doys_index))


sBands = [b for b in staticBands if b in bands]
sBandsIndex = [bands.index(x) for x in sBands]
ssBands = [b for b in annualBands if b in bands]
ssBandsIndex = [bands.index(x) for x in ssBands]
bands = [b for b in bands if b not in sBands + ssBands]
allBands = bands + ssBands + sBands
setBandScaling(allBands)

print('Map generation based on model {}:'.format(modelFile))
saved_model = load_model(modelFolder + modelFile)

featuresDict = {'DOY': tf.io.FixedLenFeature([num_features, patchSize * patchSize], dtype=tf.float32)}

def parse_image(example_proto):
    parsed_features = tf.io.parse_single_example(example_proto, featuresDict)
    return parsed_features

def imageDataGenerator(fileNames):
    dataset = tf.data.TFRecordDataset(fileNames, compression_type='GZIP')
    dataset = dataset.map(parse_image, num_parallel_calls=5)
    return dataset

dataset = imageDataGenerator(dataFiles)
it = iter(dataset)
output_probs = np.zeros((imageHeight, imageWidth, NC), dtype=np.float32)
i = 0
try:
    while i < patchPerRow * patchPerCol:
        pr, pc = divmod(i, patchPerRow)
        image_array = np.array(next(it)['DOY'])
        image_array = image_array.reshape(-1, numBands, patchSize, patchSize)
        image_array = image_array[doys_index,]
        if image_array.shape[0] > max_len:              # clip the observations to maximum max_len entries
            image_array = image_array[:max_len,]
        image_array_copy = image_array
        # Extract static data and pick just one copy
        s_image = image_array[0, sBandsIndex, :, :]
        # Extract annual data and pick one copy per year
        ss_image = image_array[0, ssBandsIndex, :, :]
        # Drop static and annual data from the array and keep just dynamic data
        image_array = np.delete(image_array, sBandsIndex + ssBandsIndex, 1)
        image_array = (image_array + add_to) / divide_to
        ss_image = (ss_image + add_to_ss) / divide_to_ss
        s_image = (s_image + add_to_s) / divide_to_s
        valid = np.ones((patchSize, patchSize))
        classes = np.zeros((patchSize, patchSize,NC))
        for j in range(patchSize):
            test_data = []
            dataVector = []
            annualDataVector = []
            staticDataVector = []
            for k in range(patchSize):
                if image_array_copy[0,0,j,k] == -1:
                    valid[j,k] = -1
                blob = image_array[:, :, j, k]
                outputBlob = blob[~np.all(blob[:, LSbandsIndex] <= 0, axis=1)]
                ss_output = ss_image[:, j, k]
                s_output = s_image[:, j, k]
                pad_length = max_len - outputBlob.shape[0]
                pad = np.zeros((pad_length,) + outputBlob.shape[1:])
                outputBlob = np.concatenate((pad, outputBlob), axis=0)
                dataVector.append(outputBlob)
                annualDataVector.append(ss_output)
                staticDataVector.append(s_output)
            test_data = [np.array(dataVector), np.array(annualDataVector), np.array(staticDataVector)]
            #classes[j,] = np.argmax(saved_model.predict_on_batch(test_data), axis=1)
            classes[j,] = saved_model.predict_on_batch(test_data)
        classes[valid < 0] = -1
        output_probs[pr * patchSize:(pr + 1) * patchSize, pc * patchSize:(pc + 1) * patchSize,:] = classes
        if (i % 100 == 0):
            print('{} patches read.'.format(i+1))
            i += 1
except tf.errors.OutOfRangeError:
    pass
#plt.imshow(output, cmap=cmap)
#plt.show()
output = np.argmax(output_probs, axis=2)
np.savez_compressed(outFileName+'.npz',class_map=output, class_probabilities=output_probs)

max_probs = np.max(output_probs, axis=2)

# reomve first row and first column of output that are caused by erroneous export of ecoregion blocks by GEE
# to have correct registration matching the original tif file
if projectionSource:
    output = output[2:,1:]
    max_probs = max_probs[2:,1:]
# save output map in and then max_probs in .png
if np.min(output) < 0:
    cmap = ListedColormap(["white", "aqua", "red", "lawngreen", "darkgreen", "khaki", "darkorange", "blue"])
else:
    cmap = ListedColormap(["aqua", "red", "lawngreen", "darkgreen", "khaki", "darkorange", "blue"])
fig, (ax1, ax2) = plt.subplots(dpi=600,ncols=2)
image1 = ax1.imshow(output, cmap=cmap)
ax1.axes.xaxis.set_visible(False)
ax1.axes.yaxis.set_visible(False)
image2 = ax2.imshow(max_probs, cmap="RdPu")
ax2.axes.xaxis.set_visible(False)
ax2.axes.yaxis.set_visible(False)
plt.savefig(outFileName+'.png', format='png')
# save output map and max_probabilities in georeferenced .tif format
im = Image.fromarray(output.astype(np.int16))
im.save(outFileName+'.tif')
dataset = gdal.Open(outFileName+'.tif', gdal.GA_Update)
dataset.SetGeoTransform(crs)
dataset.SetProjection(proj)
del dataset
im = Image.fromarray((32767*max_probs).astype(np.int16))
im.save(outFileName+'_maxprob.tif')
dataset = gdal.Open(outFileName+'_maxprob.tif', gdal.GA_Update)
dataset.SetGeoTransform(crs)
dataset.SetProjection(proj)
del dataset

print('Block {} processing time = {:.0f} seconds\n'.format(blockAIfile, time.time()-start_time))

